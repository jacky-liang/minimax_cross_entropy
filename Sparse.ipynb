{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe, CharNGram\n",
    "\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from minimax_entropy import MinimaxEntropyEstimator\n",
    "\n",
    "# https://github.com/pytorch/text\n",
    "# http://cogcomp.org/Data/QA/QC/\n",
    "# note, increasing dim decreases loss (we can see that disimilar things in hd space are very far away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "batch_size = 100\n",
    "learning_rate = 1\n",
    "dim = 1000\n",
    "classes = 6\n",
    "class_dict = {'loc':0, 'hum':1, 'num':2, 'abbr':3, 'enty':4, 'desc':5}\n",
    "word_dict = {}\n",
    "alph = 'abcdefghijklmnopqrstuvwxyz#.:-'\n",
    "letter_vecs = 2 * (np.random.randn(len(alph), N) < 0) - 1\n",
    "\n",
    "entro = MinimaxEntropyEstimator('poly_coeff_entro.mat', n_samples, gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(url):\n",
    "    texts, labels = [], []\n",
    "    with open(url) as f:\n",
    "        content = f.readlines()\n",
    "        content = list(set(content))\n",
    "        for i in range(len(content)):\n",
    "            l = content[i].lower()\n",
    "            l = l.replace('\\'', '')\n",
    "            l = l.replace('``', '')\n",
    "            l = l.replace('?', '')\n",
    "            l = l.split(' ')\n",
    "            l = filter(None, l)\n",
    "            \n",
    "            label = l[0].split(':')[0]\n",
    "            l = l[1:len(l)-1]\n",
    "            texts.append(l)\n",
    "            labels.append(class_dict[label])\n",
    "    return texts, labels\n",
    "\n",
    "def batch_data(texts, labels, batch_size):\n",
    "    text_batches, label_batches = [], []\n",
    "    previ = 0\n",
    "    for i in range(batch_size,len(texts), batch_size):\n",
    "        text_batches.append(texts[previ:i])\n",
    "        label_batches.append(np.array(labels[previ:i]))\n",
    "        previ = i\n",
    "    remainder = len(texts)% batch_size\n",
    "    print ('remainder', remainder)\n",
    "    print ('texts length', len(text_batches))\n",
    "    if len(texts) % batch_size != 0:\n",
    "        ri = len(texts)-remainder\n",
    "        text_batches.append(texts[ri:])\n",
    "        label_batches.append(np.array(labels[ri:]))\n",
    "    return text_batches, label_batches\n",
    "\n",
    "# def encode_text(texts):\n",
    "#     sentences = np.zeros((len(texts), dim))\n",
    "#     for i in range(len(texts)):\n",
    "#         s_vec = np.ones(dim)\n",
    "#         for j in range(len(texts[i])):\n",
    "#             # we try this first before we build from character space to word space\n",
    "#             if texts[i][j] not in word_dict:\n",
    "#                 word_dict[texts[i][j]] = 2 * (np.random.randn(dim) < 0) - 1\n",
    "#             s_vec = s_vec*np.roll(word_dict[texts[i][j]], j)\n",
    "#         sentences[i] = s_vec\n",
    "#     return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remainder 0\n",
      "texts length 4\n",
      "remainder 82\n",
      "texts length 53\n"
     ]
    }
   ],
   "source": [
    "testurl = 'data/raw/TREC_10.label'\n",
    "trainurl = 'data/raw/train_5500.label'\n",
    "test_texts, test_labels = read_data(testurl)\n",
    "train_texts, train_labels = read_data(trainurl)\n",
    "\n",
    "test_batches, ytest_batches = batch_data(test_texts, test_labels, batch_size)\n",
    "train_batches, y_batches = batch_data(train_texts, train_labels, batch_size)\n",
    "\n",
    "# test_vecs = encode_text(test_texts)\n",
    "# train_vecs = encode_text(train_texts)\n",
    "test_vecs = [encode_text(text) for text in test_batches]\n",
    "train_vecs = [encode_text(text) for text in train_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'was', 'mark', 'johnson', 'referring', 'to', 'when', 'he', 'said', ':', 'i', 'still', 'can', 't', 'believe', 'it-', 'we', 'beat', 'the', 'russians'], ['what', 'country', 'imposed', 'the', 'berlin', 'blockade', 'in', '1948'], ['what', 'was', 'the', 'species', 'of', 'winnie', 'the', 'pooh'], ['how', 'wide', 'is', 'the', 'atlantic', 'ocean'], ['mississippi', 'is', 'nicknamed', 'what'], ['what', 'room', 'did', 'w.c.', 'fields', 'keep', 'his', 'library', 'in'], ['who', 'is', 'the', 'composer', 'of', 'canon', 'in', 'd', 'major'], ['describe', 'the', 'long', 'march'], ['what', 'was', 'the', 'bridge', 'of', 'san', 'luis', 'rey', 'made', 'of'], ['what', 'is', 'the', 'zodiac', 'sign', 'for', 'august', '14'], ['what', 'country', 'is', 'the', 'origin', 'of', 'the', 'band', 'the', 'creeps'], ['what', 'asian', 'leader', 'was', 'known', 'as', 'the', 'little', 'brown', 'saint'], ['what', 'famed', 'river', 'was', 'hernando', 'de', 'soto', 'the', 'first', 'european', 'to', 'see'], ['who', 'sings', 'the', 'song', 'drink', 'to', 'me', 'with', 'thine', 'eyes', 'by', 'ben', 'johnson'], ['in', 'what', 'area', 'of', 'the', 'world', 'was', 'the', 'six', 'day', 'war', 'fought'], ['what', 'is', 'the', 'name', 'of', 'the', 'president', 'of', 'garmat', 'u.s.a'], ['what', 'shakespearean', 'play', 'featured', 'shylock'], ['how', 'long', 'does', 'it', 'take', 'to', 'hike', 'the', 'entire', 'appalachian', 'trail'], ['what', 'is', 'the', 'dot', 'on', 'an', 'the', 'letter', 'i', 'called'], ['what', 'longtime', 'game', 'show', 'host', 'dropped', 'dead', 'while', 'jogging', 'in', 'central', 'park', 'in', '1984'], ['what', 'bowl', 'game', 'began', 'as', 'an', 'east-west', 'contest', 'between', 'michigan', 'and', 'stanford', 'in', '192'], ['how', 'long', 'did', 'the', 'charles', 'manson', 'murder', 'trial', 'last'], ['who', 'holds', 'the', 'nfl', 'record', 'for', 'most', 'touchdowns', 'in', 'a', 'season'], ['what', 'is', 'the', 'origin', 'of', 'the', 'word', 'nevermind'], ['what', 'was', 'the', 'highest', 'mountain', 'on', 'earth', 'before', 'mount', 'everest', 'was', 'discovered'], ['what', 'famous', 'events', 'have', 'happened', 'on', 'march', '27'], ['which', 'member', 'of', 'charlie', 's', 'angels', 'sang', 'vocals', 'for', 'josie', 'and', 'the', 'pussycats'], ['how', 'many', 'beatles', 'records', 'went', '#1'], ['what', 'age', 'followed', 'the', 'bronze', 'age'], ['what', 'was', 'the', 'name', 'of', 'the', 'lawyer', 'who', 'represented', 'randy', 'craft'], ['what', 'is', 'the', 'difference', 'between', 'a', 'preface', 'and', 'a', 'foreword'], ['what', 'las', 'vegas', 'hotel', 'burned', 'in', 'november', ',', '1980', ',', 'with', 'the', 'loss', 'of', '84', 'lives'], ['italy', 'is', 'the', 'largest', 'producer', 'of', 'what'], ['when', 'was', 'dick', 'clark', 'born'], ['what', 'is', 'agent', 'orange'], ['what', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'national', 'bureau', 'of', 'investigation'], ['where', 'do', 'the', 'adventures', 'of', 'the', 'swiss', 'family', 'robinson', 'take', 'place'], ['what', 'does', 'the', 'name', 'sheri', 'mean'], ['how', 'many', 'feet', 'are', 'there', 'in', 'a', 'fathom'], ['what', 'does', 'idle', 'mean'], ['who', 'protects', 'dc', 'comics', 'realm', 'of', 'dreams'], ['when', 'was', 'the', 'nfl', 'established'], ['what', 'was', 'the', 'name', 'of', 'the', 'confederate', 'mounted', 'guerrilla', 'group', 'with', 'which', 'jesse', 'james', 'and', 'coleman', 'younger', 'had', 'ridden'], ['what', 'tv', 'detective', 'did', 'craig', 'stevens', 'play'], ['what', 'does', 'jessica', 'mean'], ['what', 'broadway', 'musical', 'was', 'inspired', 'by', 'cervantes', 's', 'don', 'quixote'], ['what', 's', 'the', 'sacred', 'river', 'of', 'india'], ['in', 'which', 'kevin', 'costner', 'movie', 'did', 'sioux', 'indians', 'play', 'a', 'role'], ['how', 'many', 'electoral', 'college', 'votes', 'does', 'colorado', 'have'], ['how', 'do', 'i', 'get', 'my', 'lan', 'card', 'activated', 'so', 'that', 'it', 'can', 'hook', 'up', 'to', 'another', 'computer', 'without', 'using', 'a', 'hub'], ['what', 'movie', 'did', 'steven', 'spielberg', 'direct', 'in', '1975'], ['at', 'christmas', 'time', ',', 'what', 'is', 'the', 'traditional', 'thing', 'to', 'do', 'under', 'the', 'mistletoe'], ['what', '1895', 'h.g.', 'wells', 'novel', 'was', 'written', 'under', 'the', 'title', 'the', 'chronic', 'argonauts'], ['what', 'are', 'the', 'words', 'to', 'the', 'canadian', 'national', 'anthem'], ['what', 'is', 'the', 'difference', 'between', 'hair', 'and', 'fur'], ['on', 'what', 'day', 'were', 'john', 'f', 'and', 'jackie', 'kennedy', 'married'], ['what', 'landmark', 'italian', 'restaurant', 'can', 'be', 'found', 'at', '239', 'west', '48th', 'street', ',', 'new', 'york', 'city'], ['what', 'do', 'manatees', 'eat'], ['name', 'a', 'golf', 'course', 'in', 'myrtle', 'beach'], ['what', 'exactly', 'is', 'radiation'], ['what', 'country', ',', 'after', 'canada', 'and', 'mexico', ',', 'is', 'closest', 'to', 'the', 'u.s.'], ['what', 'is', 'the', 'time', 'it', 'takes', 'a', 'typist', 'to', 'type', 'a', 'screenplay', 'that', 'is', '100', 'pages', 'long'], ['when', 'did', 'aldous', 'huxley', 'write', ',', 'brave', 'new', 'world'], ['who', 'used', 'auh2o', 'as', 'an', 'election', 'slogan'], ['what', 'is', 'leukemia'], ['what', 'is', 'time'], ['who', 'is', 'the', 'leader', 'of', 'brunei'], ['who', 'runs', 'andy', 'capp', 's', 'favorite', 'pub'], ['how', 'many', 'different', 'kinds', 'of', 'ice', 'cream', 'are', 'there'], ['what', 'is', 'the', 'name', 'of', 'the', 'woman', 'who', 'was', 'with', 'john', 'belushi', 'when', 'he', 'died'], ['what', 'amount', 'of', 'money', 'did', 'the', 'philippine', 'ex-dictator', 'marcos', 'steal', 'from', 'the', 'treasury'], ['what', 'college', 'produced', 'the', 'most', 'winning', 'super', 'bowl', 'quarterbacks'], ['how', 'do', 'they', 'find', 'an', 'epicenter'], ['where', 'does', 'chocolate', 'come', 'from'], ['how', 'many', 'people', 'died', 'in', 'the', 'vietnam', 'war'], ['what', 'party', 'was', 'winston', 'churchill', 'a', 'member', 'of'], ['what', 'are', 'field', 'effect', 'transistors'], ['how', 'many', 'holes', 'are', 'there', 'in', 'a', 'tenpin', 'bowling', 'ball'], ['who', 'was', 'the', 'first', 'man', 'to', 'return', 'to', 'space'], ['what', 'is', 'the', 'origin', 'of', 'boxing', 'day'], ['what', 'does', 'the', 'name', 'calder', 'mean'], ['how', 'many', 'zip', 'codes', 'are', 'there', 'in', 'the', 'u.s.'], ['how', 'can', 'i', 'find', 'a', 'list', 'of', ',', 'fax', 'and', 'or', 'email', ',', 'addresses', 'for', 'human', 'resource', 'departments', 'in', 'massachusetts'], ['what', 'two', 'european', 'countries', 'are', 'joined', 'by', 'the', 'gran', 'san', 'bernardo', 'pass'], ['how', 'many', 'people', 'in', 'the', 'usa', 'say', 'their', 'number', 'one', 'source', 'of', 'information', 'is', 'the', 'newspaper'], ['what', 'is', 'the', 'average', 'time', 'to', 'kiss', 'somene'], ['what', 'day', 'is', 'august', '13', ',', '1971'], ['what', 'city', 'is', 'served', 'by', 'logan', 'international', 'airport'], ['what', 'is', 'the', 'longest', 'english', 'word'], ['how', 'many', 'fingers', 'are', 'used', 'to', 'draw', 'a', 'bow'], ['what', 'is', 'the', 'origin', 'of', 'barbeque'], ['what', 'product', 'is', 'for', 'kids', ',', 'and', 'not', 'for', 'silly', 'rabbits'], ['stuart', 'hamblen', 'is', 'considered', 'to', 'be', 'the', 'first', 'singing', 'cowboy', 'of', 'which', 'medium'], ['what', 'is', 'a', 'fear', 'of', 'black', 'people'], ['what', 'tv', 'series', 'features', 'the', 'adventures', 'of', 'a', 'married', 'couple', 'named', 'jonathan', 'and', 'jennifer'], ['what', 'city', 'is', 'found', 'in', 'the', 'city', 'of', 'oz'], ['who', 'is', 'colin', 'powell'], ['what', 'kind', 'of', 'wine', 'is', 'spumante'], ['what', 'animal', 'dined', 'on', 'bread', 'and', 'oysters', 'with', 'a', 'carpenter'], ['where', 'do', 'lobsters', 'like', 'to', 'live']]\n",
      "[4 0 4 2 4 0 1 5 4 4 0 1 0 1 0 1 4 2 4 1 4 2 1 5 0 4 1 2 4 1 5 0 4 2 5 3 0\n",
      " 5 2 5 1 2 1 1 3 4 0 4 2 5 4 4 4 5 5 2 0 4 4 5 0 2 2 1 5 5 1 1 2 1 2 1 5 0\n",
      " 2 1 5 2 1 5 5 2 5 0 2 2 2 0 4 2 5 4 4 4 4 0 1 4 4 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_batches[0])\n",
    "print(y_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _g(p, q):\n",
    "    if (q.data == 1).all():\n",
    "        return - torch.log(p + eps) / _denom\n",
    "    return - torch.log(1 - p + eps) / _denom\n",
    "    \n",
    "def cross_entro_loss(dist_p, dist_q):\n",
    "    H = Variable(torch.zeros(1)).double()\n",
    "    for i in range(len(dist_p)):\n",
    "        H += _g(dist_p[i], dist_q[i])\n",
    "    return H\n",
    "    \n",
    "    \n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "\n",
    "# loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "loss_fn = cross_entro_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 14041.3473881\n",
      "epoch 1 loss 6107.76133266\n",
      "epoch 2 loss 5659.45916929\n",
      "epoch 3 loss 5594.59833694\n",
      "epoch 4 loss 5566.43250907\n",
      "epoch 5 loss 5538.4982495\n",
      "epoch 6 loss 5506.95097953\n",
      "epoch 7 loss 5469.35862698\n",
      "epoch 8 loss 5422.42187241\n",
      "epoch 9 loss 5341.80638848\n",
      "epoch 10 loss 5216.24056382\n",
      "epoch 11 loss 5097.9389819\n",
      "epoch 12 loss 4956.52664968\n",
      "epoch 13 loss 4791.19585034\n",
      "epoch 14 loss 4550.72342295\n",
      "epoch 15 loss 4392.7484402\n",
      "epoch 16 loss 4097.06864261\n",
      "epoch 17 loss 4411.71983071\n",
      "epoch 18 loss 3819.08684796\n",
      "epoch 19 loss 4144.6363096\n",
      "epoch 20 loss 3974.57358793\n",
      "epoch 21 loss 3971.03717005\n",
      "epoch 22 loss 3389.88297621\n",
      "epoch 23 loss 3630.92359439\n",
      "epoch 24 loss 3185.18851465\n",
      "epoch 25 loss 3583.5710262\n",
      "epoch 26 loss 3186.11296981\n",
      "epoch 27 loss 3671.11579543\n",
      "epoch 28 loss 3155.09699747\n",
      "epoch 29 loss 3521.79729935\n",
      "epoch 30 loss 3179.94370779\n",
      "epoch 31 loss 3072.65998912\n",
      "epoch 32 loss 3102.35215535\n",
      "epoch 33 loss 3052.60680786\n",
      "epoch 34 loss 2893.02916827\n",
      "epoch 35 loss 2947.03834816\n",
      "epoch 36 loss 2661.64934994\n",
      "epoch 37 loss 3054.30155682\n",
      "epoch 38 loss 2803.83460231\n",
      "epoch 39 loss 2639.45884001\n",
      "epoch 40 loss 2845.40949652\n",
      "epoch 41 loss 2477.67857052\n",
      "epoch 42 loss 2999.62802657\n",
      "epoch 43 loss 2313.57778583\n",
      "epoch 44 loss 2957.61040937\n",
      "epoch 45 loss 2577.55345174\n",
      "epoch 46 loss 2755.63436416\n",
      "epoch 47 loss 2156.7952247\n",
      "epoch 48 loss 2829.41890673\n",
      "epoch 49 loss 2385.82200577\n",
      "epoch 50 loss 2570.92032645\n",
      "epoch 51 loss 2521.60570012\n",
      "epoch 52 loss 2261.23071674\n",
      "epoch 53 loss 2388.74474472\n",
      "epoch 54 loss 2313.88752544\n",
      "epoch 55 loss 2394.04072924\n",
      "epoch 56 loss 2296.91099243\n",
      "epoch 57 loss 2152.42102503\n",
      "epoch 58 loss 2074.21446548\n",
      "epoch 59 loss 2140.94919934\n",
      "epoch 60 loss 2429.36171642\n",
      "epoch 61 loss 2049.6000207\n",
      "epoch 62 loss 2430.14007615\n",
      "epoch 63 loss 2252.96100356\n",
      "epoch 64 loss 1960.83192967\n",
      "epoch 65 loss 2123.69645536\n",
      "epoch 66 loss 2052.12434259\n",
      "epoch 67 loss 2109.1878969\n",
      "epoch 68 loss 2172.33379515\n",
      "epoch 69 loss 2136.56286157\n",
      "epoch 70 loss 1957.55476319\n",
      "epoch 71 loss 2366.51416091\n",
      "epoch 72 loss 2097.45691591\n",
      "epoch 73 loss 2048.17303643\n",
      "epoch 74 loss 1996.83374137\n",
      "epoch 75 loss 1495.70081735\n",
      "epoch 76 loss 2007.79044316\n",
      "epoch 77 loss 1765.92204764\n",
      "epoch 78 loss 1980.12736952\n",
      "epoch 79 loss 1615.71455944\n",
      "epoch 80 loss 2031.21540889\n",
      "epoch 81 loss 1550.22035452\n",
      "epoch 82 loss 2050.78392142\n",
      "epoch 83 loss 1947.71364947\n",
      "epoch 84 loss 1779.3601089\n",
      "epoch 85 loss 2133.48856396\n",
      "epoch 86 loss 2010.74410158\n",
      "epoch 87 loss 1825.02218334\n",
      "epoch 88 loss 1797.9871548\n",
      "epoch 89 loss 1851.60228927\n",
      "epoch 90 loss 1766.9320973\n",
      "epoch 91 loss 1504.4119169\n",
      "epoch 92 loss 1892.3777491\n",
      "epoch 93 loss 1791.25928377\n",
      "epoch 94 loss 1618.89327251\n",
      "epoch 95 loss 1809.60162151\n",
      "epoch 96 loss 1558.74341782\n",
      "epoch 97 loss 1704.20693647\n",
      "epoch 98 loss 1404.52202784\n",
      "epoch 99 loss 1549.40451849\n"
     ]
    }
   ],
   "source": [
    "_denom = np.log(np.e)\n",
    "eps = 1e-12\n",
    "\n",
    "def _g(p, q):\n",
    "    if (q.data == 1).all():\n",
    "        return - torch.log(p + eps) / _denom\n",
    "    return - torch.log(1 - p + eps) / _denom\n",
    "    \n",
    "def cross_entro_loss(dist_p, dist_q):\n",
    "    H = Variable(torch.zeros(1)).double()\n",
    "    for i in range(len(dist_p)):\n",
    "        H += _g(dist_p[i], dist_q[i])\n",
    "    return H\n",
    "\n",
    "def accuracy(yhat, labels):\n",
    "    maxs, indices = torch.max(torch.exp(yhat), 1)\n",
    "    for i in range(y.size()):\n",
    "        \n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "\n",
    "loss_fn = cross_entro_loss\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = batch_size, dim, 100, 6\n",
    "\n",
    "learning_rate = 1e-4\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "batch_idx = 0\n",
    "\n",
    "x = Variable(torch.FloatTensor(train_vecs[batch_idx]), requires_grad=True)\n",
    "\n",
    "yhot = np.zeros((y_batches[batch_idx].shape[0], D_out))\n",
    "yhot[[idx for idx in range(yhot.shape[0])], y_batches[batch_idx]] = 1\n",
    "y = Variable(torch.DoubleTensor(yhot), requires_grad=False)\n",
    "\n",
    "for epoch in range(10):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = Variable(torch.zeros(1), requires_grad=True).double()\n",
    "    for i in range(len(train_vecs[batch_idx])):\n",
    "        yhat = y_pred[i] + y_pred[i].min().abs()\n",
    "        norm = torch.norm(yhat, 2)\n",
    "        yhat = yhat.div(norm)\n",
    "        yhat = yhat.div(yhat.sum())\n",
    "        yhat = yhat.double()\n",
    "\n",
    "        loss += loss_fn(yhat, y[i])\n",
    "\n",
    "    \n",
    "    print ('epoch', str(epoch), 'loss', loss.data[0])\n",
    "    \n",
    "    losses.append(loss.data[0])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcHWWV//HPMWERURPIiJhEK0iEAUYFI0ZhFAEhUEiY\nUVlGJQgaRxh3xUJhUAQs1AFBFEV29EdARMlYIGYARUZBYlBAlkmAAhLZE+JORM7vj/O0XLvv7e70\nmu76vl+vfvWtp5b7VC7c0895ljJ3R0REpNWzRrsCIiKy7lFwEBGRHhQcRESkBwUHERHpQcFBRER6\nUHAQEZEeFBxERKQHBQcREelBwUFERHqYONoVGCgzexS4b7TrISIyxrzE3f+hr4PGbHAA7nP3WaNd\nCRGRscTMFvfnOKWVRESkBwUHERHpQcFBRER6UHAQEZEeFBxERKSHPoODmZ1jZo+Y2W1t9n3UzNzM\npqRtM7PTzGyZmd1iZju0HDvPzJamn3kt5a8ys1vTOaeZmQ3VzYmIyMD0p+VwHjCne6GZTQf2AO5v\nKd4LmJl+5gNnpGM3AY4FXgPsCBxrZpPTOWcA72k5r8d7iYjIyOozOLj7dcDKNrtOAY4EWp8zOhe4\nwMMNwCQz2xzYE1jk7ivdfRWwCJiT9j3P3W/weF7pBcB+g7ul3mVF9f6sqA4YzvcQERnrBtTnYGZz\ngRXu/qtuu6YCD7RsL09lvZUvb1M+nN4LvG2Y30NEZExb6xnSZrYR8EkipTSizGw+ka4CmDLAy6wB\nNhiaGomIjE8DaTm8FJgB/MrMamAasMTMXgisAKa3HDstlfVWPq1NeVvufqa7z0rLZjw2gLpDBIf1\nB3iuiEgjrHVwcPdb3f0F7p65e0akgnZw94eAhcDBadTSbGC1uz8IXAXsYWaTU0f0HsBVad9vzWx2\nGqV0MHD5EN1bJ0+i4CAi0qv+DGW9CPgZsJWZLTezw3o5/ArgHmAZ8A3gcAB3Xwl8Frgp/RyXykjH\nnJXOuRu4cmC30m9KK4mI9KHPPgd3P6iP/VnLaweO6HDcOcA5bcoXA9v1VY8htAaY3OdRIiIN1sQZ\n0koriYj0oYnBQWklEZE+NDU4qOUgItKLJgYHpZVERPrQxOCgtJKISB+aGhzUchAR6UUTg4PSSiIi\nfWhicFgDbJAVlZ4bISLSQVODAwxg0UERkaZoYnB4Mv1WaklEpIMmBoeuloNGLImIdNDk4KCWg4hI\nB00MDkoriYj0oYnBQWklEZE+NDk4qOUgItJBE4OD0koiIn1oYnBQWklEpA9NDg5qOYiIdNDE4KC0\nkohIH5oYHJRWEhHpQxODg1oOIiJ9aGJwUJ+DiEgfmhwclFYSEemgz+BgZueY2SNmdltL2RfM7E4z\nu8XMvmtmk1r2HWVmy8zsLjPbs6V8TipbZmZFS/kMM7sxlV9sZsP9F73SSiIifehPy+E8YE63skXA\ndu7+cuD/gKMAzGwb4EBg23TOV81sgplNAL4C7AVsAxyUjgU4CTjF3bcEVgGHDeqO+qa0kohIH/oM\nDu5+HbCyW9kP3f2ptHkDMC29ngsscPcn3f1eYBmwY/pZ5u73uPsaYAEw18wM2BW4NJ1/PrDfIO+p\nL0oriYj0YSj6HA4FrkyvpwIPtOxbnso6lW8KPNESaLrK2zKz+Wa22MwWA1MGWF+llURE+jCo4GBm\nnwKeAr41NNXpnbuf6e6z3H0W8NgAL6O0kohIHwYcHMzsEGAf4O3u7ql4BTC95bBpqaxT+ePAJDOb\n2K182NRl/lfgaZRWEhHpaEDBwczmAEcC+7r7H1t2LQQONLMNzGwGMBP4OXATMDONTFqf6LRemILK\ntcBb0/nzgMsHditr5UnUchAR6ag/Q1kvAn4GbGVmy83sMOB04LnAIjP7pZl9DcDdfw1cAtwO/AA4\nwt3/mvoU/gO4CrgDuCQdC/AJ4CNmtozogzh7SO+wvTWo5SAi0tHEvg5w94PaFHf8Anf3E4AT2pRf\nAVzRpvweYjTTSFqDWg4iIh01cYY0KK0kItKrpgYHpZVERHrR5OCgloOISAdNDQ5KK4mI9KKpwUFp\nJRGRXjQ5OKjlICLSQVODg9JKIiK9aGpwUFpJRKQXTQ4OajmIiHTQ1OCgtJKISC+aGhyUVhIR6UWT\ng4NaDiIiHTQ1OCitJCLSi6YGB6WVRER60eTgoJaDiEgHTQ0OSiuJiPSiqcFhDTAhK6oJo10REZF1\nUZODA6j1ICLSVlODw5Ppt4KDiEgbTQ0OXS0HjVgSEWmjqcFBLQcRkV40NTioz0FEpBd9BgczO8fM\nHjGz21rKNjGzRWa2NP2enMrNzE4zs2VmdouZ7dByzrx0/FIzm9dS/iozuzWdc5qZ2VDfZBtKK4mI\n9KI/LYfzgDndygrganefCVydtgH2Amamn/nAGRDBBDgWeA2wI3BsV0BJx7yn5bzu7zUclFYSEelF\nn8HB3a8DVnYrngucn16fD+zXUn6BhxuASWa2ObAnsMjdV7r7KmARMCfte5673+DuDlzQcq3hpLSS\niEgvJg7wvM3c/cH0+iFgs/R6KvBAy3HLU1lv5cvblLdlZvOJFgnAlAHWHZRWEhHp1aA7pNNf/D4E\ndenPe53p7rPcfRbw2CAupbSSiEgvBhocHk4pIdLvR1L5CmB6y3HTUllv5dPalA83pZVERHox0OCw\nEOgacTQPuLyl/OA0amk2sDqln64C9jCzyakjeg/gqrTvt2Y2O41SOrjlWsNJaSURkV702edgZhcB\nuwBTzGw5MeqoBC4xs8OA+4D90+FXAHsDy4A/Au8CcPeVZvZZ4KZ03HHu3tXJfTgxIurZwJXpZ7gp\nrSQi0os+g4O7H9Rh125tjnXgiA7XOQc4p035YmC7vuoxxJRWEhHpRdNnSCutJCLSRlODg9JKIiK9\naGpwUMtBRKQXTQ8OajmIiLTR1OCgtJKISC+aGhyeSr+VVhIRaaORwaEucydSS2o5iIi00cjgkDyJ\ngoOISFtNDg5rUFpJRKStpgcHtRxERNpocnBQWklEpIMmBwellUREOmh6cFDLQUSkjSYHB6WVREQ6\naHJwUFpJRKSDpgcHtRxERNpocnBQWklEpIMmBwellUREOmh6cFDLQUSkjSYHB6WVREQ6aHJwUFpJ\nRKSDJgcHtRxERDoYVHAwsw+b2a/N7DYzu8jMNjSzGWZ2o5ktM7OLzWz9dOwGaXtZ2p+1XOeoVH6X\nme05uFvqN/U5iIh0MODgYGZTgQ8As9x9O2ACcCBwEnCKu28JrAIOS6ccBqxK5aek4zCzbdJ52wJz\ngK+a2YSB1mstKK0kItLBYNNKE4Fnm9lEYCPgQWBX4NK0/3xgv/R6btom7d/NzCyVL3D3J939XmAZ\nsOMg69UfSiuJiHQw4ODg7iuALwL3E0FhNfAL4Al373pG83Jgano9FXggnftUOn7T1vI25wynNcB6\nWVHZCLyXiMiYMpi00mTir/4ZwIuA5xBpoWFjZvPNbLGZLQamDPJya9JvtR5ERLoZTFppd+Bed3/U\n3f8CXAbsBExKaSaAacCK9HoFMB0g7X8+8HhreZtz/o67n+nus9x9FvDYIOoOkVYCBQcRkR4GExzu\nB2ab2Uap72A34HbgWuCt6Zh5wOXp9cK0Tdp/jbt7Kj8wjWaaAcwEfj6IevWXWg4iIh1M7PuQ9tz9\nRjO7FFgCPAXcDJwJVMACMzs+lZ2dTjkbuNDMlgEriRFKuPuvzewSIrA8BRzh7n8daL3WQldw0Igl\nEZFuBhwcANz9WODYbsX30Ga0kbv/GXhbh+ucAJwwmLoMgNJKIiIdNHmGtNJKIiIdKDgorSQi0kOT\ng4PSSiIiHTQ5OCitJCLSgYKD0koiIj00OTgorSQi0kGTg4NaDiIiHSg4qOUgItJDk4OD0koiIh00\nOTgorSQi0oGCg1oOIiI9NDk4KK0kItJBk4OD0koiIh0oOKjlICLSg4KDgoOISA+NDQ51mT9NPFxI\naSURkW4aGxySNajlICLSQ9ODw5MoOIiI9ND04LAGpZVERHpQcFDLQUSkh6YHB6WVRETaaHpwUFpJ\nRKSNQQUHM5tkZpea2Z1mdoeZvdbMNjGzRWa2NP2enI41MzvNzJaZ2S1mtkPLdeal45ea2bzB3tRa\nUMtBRKSNwbYcTgV+4O5bA68A7gAK4Gp3nwlcnbYB9gJmpp/5wBkAZrYJcCzwGmBH4NiugDIC1Ocg\nItLGgIODmT0feD1wNoC7r3H3J4C5wPnpsPOB/dLrucAFHm4AJpnZ5sCewCJ3X+nuq4BFwJyB1mst\nPQZsnRXVhBF6PxGRMWEwLYcZwKPAuWZ2s5mdZWbPATZz9wfTMQ8Bm6XXU4EHWs5fnso6lY+Ec4GX\nAPuO0PuJiIwJgwkOE4EdgDPcfXvgDzyTQgLA3R3wQbzH3zGz+Wa22MwWA1OG4JKXA/cBHxqCa4mI\njBuDCQ7LgeXufmPavpQIFg+ndBHp9yNp/wpgesv501JZp/Ie3P1Md5/l7rOIlNCg1GX+FPBl4PVZ\nUe3Q1/EiIk0x4ODg7g8BD5jZVqloN+B2YCHQNeJoHvHXOan84DRqaTawOqWfrgL2MLPJqSN6j1Q2\nUs4Cfo9aDyIifzPY0UrvB75lZrcArwROBErgTWa2FNg9bQNcAdwDLAO+ARwO4O4rgc8CN6Wf41LZ\niKjLfDXR93BgVlSbj9T7ioisyyy6BcYeM1uc0kuDlhXVTOAu4HTgY3WZr+njFBGRMam/350KDklW\nVAuAA4DVRArsCiJgLKvL/HdD9T4iIqOpv9+dE0eiMmPEO4ELgLcRczPe2bUjK6qHiXTY0vRzJzHh\nb1ld5n8Z+aqKiAwvtRzayIpqfWBrYjb3lt1+v6jl0L8AvwZuBpYA/wvcUpf5X4ejXiIig6W00jDJ\nimpjInBsA2wLbJ9+uuZd/Ba4HrgQuEz9FyKyLlFwGEFZURnwYmBn4J+JJUEyYo7HWcDJdZk/PmoV\nFBFJFBxGUVZUzyLma7wP2Ad4HPgAcHFd5mPzH1xExgUFh3VEVlSvIFoPs4gRUEfUZV6PaqVEpLH6\n+93Z9If9DLu6zH8FzAY+AuwC3J4V1aeyotJDhkRknaWWwwjKiurFwMnAW4ghse+uy/y60a2ViDSJ\n0krrsKyo9gS+AmwBnAAcp/kSIjISlFZah9VlfhWxFtW5wNHAT7KimjG6tRIReYZaDqMsK6r9gTOJ\nCXX/Upf59aNcJREZx9RyGCPqMr8EeDWwErg6K6qDR7lKIiIKDuuCusyXEiOargfOz4rqE6NcJRFp\nOAWHdURd5quAOcACoMyK6l9GuUoi0mAKDuuQNGLpXcCNwIVZUf3TKFdJRBpKwWEdU5f5n4F/JRbw\nW5gV1ZQ+ThERGXIKDuugusx/QzxTYnPgqqyopo9ylUSkYRQc1lF1mf+caEHMBBZnRfX6Ua6SiDSI\ngsM6rC7zK4AdgSeIYa6Hdj8mK6r5WVF9Oy0bLiIyJBQc1nF1md9JBIgfAV/LiurlXfuyotoCOBV4\nK6DOaxEZMgoOY0Bd5quBg4gWxLlZUa2XWgpfBZ4CniaefS0iMiQUHMaIuswfIx4etANwJLA/8cS5\nTwI/Bt6m1JKIDJVBBwczm2BmN5vZ99P2DDO70cyWmdnFZrZ+Kt8gbS9L+7OWaxyVyu8ysz0HW6fx\nqi7z7wAXA8cCpwOLidbDt4GtgO26js2KarusqL6cFdWmo1FXERnbhqLl8EHgjpbtk4BT3H1LYBVw\nWCo/DFiVyk9Jx2Fm2wAHAtsSM4S/amYThqBe49X7ifTSJsB76zL/K3AZLamlrKgmABcC/wH8LCuq\nLUepriIyRg0qOJjZNCAnHoOJmRmwK3BpOuR8Yrw+wNy0Tdq/Wzp+LrDA3Z9093uBZUQHrLRRl/mj\nwF7Av9ZlviSVPQxcxzOppfcRS4J/jggiN2RF9c8tfRUiIr2aOMjzv0Tkv5+btjcFnnD3p9L2cmBq\nej0VeADA3Z8ys9Xp+KnADS3XbD3n75jZfGB+2mzszOG6zH8B/KJb8beJBwi9ETgeWAR8CjiHeHb1\n3544lxXVamB2GgklItLDgFsOZrYP8Ii7d/+SGjbufqa7z0prkT82Uu87RlwGOPBdYCPg/XWZe13m\ny4DXAh8GjiECx3OBd45WRUVk3TeYlsNOwL5mtjewIfA8Ysz9JDObmFoP04AV6fgVwHRguZlNBJ4P\nPN5S3qX1HOmnuswfyorqOuANwOfqMr+rZd/jRCsPgKyoZgMHZEV1dF3mY/NpTyIyrAbccnD3o9x9\nmrtnRIfyNe7+duBaYlIWwDzg8vR6Ydom7b/G4zF0C4ED02imGcRyET8faL0a7lTi3/+EPo67BHgp\nsP2w10hExqTB9jm08wlggZkdD9wMnJ3KzwYuNLNlxFPPDgRw91+b2SXA7cSEriPc/a/DUK9xry7z\n7xJppb5cRgyBPQBYMtD3SwsC/inNwRCRcUTPkG6orKiuBLYGtqjL3LOi2hg4Cvh6Xeb39+P8icDd\nwNK6zHcf3tqKyFDRM6SlL5cAGfDqrKieBZxHzLY+vvWgrKjWz4rq2jaL/uXAi4Hd0hpPIjKOKDg0\n1/eAvxDLcHwKeAuwFDgwK6rNW447GNgF+FxWVBu1lL+XGDHmPNOXBEBWVCdkRXWJ5lSIjF0KDg2V\nnln9Q+A9wHHEjOq9iX6owwGyolqPaE2sAF5ABASyoppBms2ernFIan2QFdW2QEHM1j5w5O5IRIaS\ngkOzXUwMQb4JmJ/mRPw38O9ZUT0beAcwgwgK1wJHpvL3EC2Gs4BzifTSrumaJwK/B24FPt+ttSEi\nY4SCQ7N9mxhdNjc9uxpi3aspwCFEumkJMcP6M8ALiVbFocD36zJ/gBiqvAo4NCuqnYB9iXWzjiDm\nrHx8pG5GRIaORivJ30n9BEuIkUwbAvvVZX552vdj4HVE6mnvusyvTOWnA+8mWgtTgZl1mf8hK6pL\ngH2ArVIgEZFRptFKMiBpxvQpRGD4FTFJsctniMBwH9HX0OVcYANgFvCZusz/kMqPJP4b+2pWVM8f\n5qqLyBAajklwMvYtIFbL/XK35TWuJZ4j8dO0VHiXJcSEx+cQC/0BUJd5nRXV0cAXgLuzojoO+Aaw\nOfAyYgmVn/VnXoWIjCyllWRIZEX1Qog1ntrsexXweZ7ptO7uHuBK4Jg0iqqv95pI9JV8RyvLiqyd\n/n53KjjIiEh9GXsC/0wEg7uAP6XtNxLPqKiJzvE7Olym61oHE88GuQeY1Z+A0u389wHPrcv8893K\nJwAb1GX+x7W5nshYouAgY0pWVDsD3wGeTTztbjIRTF4BHFCX+U/ScROJJw9OJDq/FwFvrsv86X6+\nz4bAg+n8KXWZP9my7wRilNbWdZn/bmjuTGTdog5pGVPqMr+e6NBeSizlcQoxx8KBC7Kiel469N+A\nLYnnU3yQmLh3TLtrZkX1T91me0P0pUwCNgZ2bjnWgIOAFwEf6m+9NY9DxisFB1lnpOGuOwNvBrK6\nzLcmZlq/GDg5tRqOAX5JzK/4GpFeOjYrqgNar5UV1d7E0/KqrtnbybuIGd9riFRWl22IYLQa+FhW\nVJv0Vd/0XIzVqdUjMq5otJKsU+oy/xPw/Zbtn2ZFdRKxYuyGRKthv65RVKn/YEtgQVZUWxIztHch\nUlSriGdW7J/2TwXelI6ZTbQ6Ppbeat/0e3/gB6n8k6lFcSSxFMgudZmvbqnu24j/h44Erh+6fwWR\n0aeWg4wFnybmXLydaDX8be5FCia7A98iVpRdSCwBcjfwT8TEvOOzolqfWESwawXaK4B/zIrqJelS\n+wI31WX+Q2Io7wfTCKwvASXwSp4JIF1y4GngzVlRvWxI71hklCk4yDqvLvM1xDOv7wY+3v3Rpmnp\nj3cSy33sQ3Q4v6ku80eIFsdLiRnchwA/qcv8bmLoLMBeKQi8hmeCzqeJVsoS4ANEgHiAZ55wSFZU\nLwW2IgLHGqL/g5b9E7UqrYxlCg4yJtRlfmtd5lvWZf4/HfZ7XeYnAq8GXleX+YNp1xXAT4iJeC8j\nWg0QQ2lrIrWUA0YKDnWZ/186bnMiZfQRIk21Z0vHeJ5+n0O0Wg7p6qfIimoukdK6PyuqC7OiOjQt\nWCgyZig4yLhSl/niuswfbdl2YgnxjYA/EosNdpVfAexG9DPcR6SguhwBvKIu8y+kYy8llgjpCgo5\ncFdqhZySrj8/K6p3EIHkLuCnwB7EI3IXqCUhY4mCg4x7dZn/FPgy8Plu8xeuJL7U9wAub01X1WX+\n57rMb2k59mdEuuqtWVE9h+j0rtKxtwL/Qzz74kLgOuCNdZkfQKxk+1Giv+IjrfXKiuoFWVFN7l7f\nrKi2Sp3ray0rqjdkRXVlVlRTupVvkRXVdVlRzRnIdaV5NAlOGit9ya8E1gd2r8v86j6O/zLRd3Eo\n8P9az0lfulcSneH7tyyB3jWH4lIiQLwBWEz0hXyKmMfxPWLxwk2A9xHDebtaK59Nwac/9zMJuI2Y\nHPjFusw/3rLvm0SH/lPAYXWZX9Cfaw5UVlT/RrTIDkqDBmQdoUlwIn1Iq8f+iJjb8JN+nHIp0VF9\nEvC71nPqMv8BMaLpLa2BIe1z4DCiU/tiYv7Fp9P1ziBSW1cSfRdd/RwnEk/buyUrqm+mL/6+nEq0\nVK4H/qNrAmBWVNsRkwe/CvwYOD8rqiOHK82V+ldOJiYcnjwc7yHDT8FBmu5w4tkUa/px7PXAI8B0\nYFH3c+oy/1Vd5n9pd2Jd5k8Q8yJeQCwN8ua6zP+tLvMPEX/pzyWG5L4s9XMcDWREkDgA+GWadNdW\nVlT7EUN1TyRGZU0kWiYQS63/HvhPoq9kARHg3tuPex6Iw4DNiKVN/j0rqrcM0/vIMBpwWsnMpgMX\nEP8ROHCmu59qZpsQfx1lxGiQ/d19lZkZ8ZfN3kTH4CHuviRdax5wdLr08e5+fj/eX2klGXFZUZ0B\n/DuRmjmnr+PbnD8DeGxt1m5KQeEi4sl6JwJfScN0u/bPBP4XWA7Mrst8TVZUXydmgx9IdJB/pi7z\nT6fjn0W0VHYCXl6X+T0t13oW4N2HC6d9mwP7pZ+NgFvSzw/rMr83HbM+sAy4n1iF93pgJvDKuszv\n6+89j6T0rPQXrav1G2rDvvCemW0ObO7uS8zsuURTeT/ir5aV7l6aWQFMdvdPmNnexIJqexNjyk91\n99ekYLKYWFfH03Ve5e69rrSp4CCjISuqVwJfB/K6zB8bwfedRCwXcgDwF+C7xDM0/gXYkVjh9jVd\n/RNZUU0jvqQnEmmzLVpnd2dFNZ3on7gZ2LUu86ezotqL6FB/AvgmkebamGht5MT/twb8H9GCejnx\nDPLfEbPWr8mK6lBidNbedZlfmeaD3EysoHs6kca7u13wGS1ZUZ1J9CV9DfhkauWNWyO+KquZXU58\n+KcDu7j7gymA/MjdtzKzr6fXF6Xj7yJGfOySjn9vKv+743p5PwUHaZysqLYB3gPMI9JTS4iW+oLu\nD03KiupLxOS8oi7zk9pcq+uL/IPABOCLRMB4jFhGvbVP4iais/07wB11mXvqs9gauISYQzIPOI4I\nFrNaljjZj/ji3SxdazlwDXA18D91mf+mTd1mAHV/g0haRuWOusx/1K38FOBVwHHt5shkRfVyYtb9\nLcSM+keA99dlfml/3ncsGtHgYGYZMXxvO+B+d5+Uyg1Y5e6TzOz7QOnu16d9VxMPbNkF2NDdj0/l\nxwB/cvcv9vGeCg7SWGnp8U3rMl/RyzGTidFPp7QbMZS+3L9PLEBoxBf/vPT87+nAW4hWxJV1mT/c\nx/v8N5GmAnhrXebfafNeWxH/v++afjYlWjyvan2GR1ZUOxHpqFPqMv9IS/k/AGcRabUftpRvDdwO\n3Als2xKUXkAssgjRgroWOLIu88Ut515FTJx8KbAFcCawA9Ff87l1qYUzVEYsOJjZxsQIiBPc/TIz\ne6IrOKT9q9x98lAEBzObD8xPm1PcPRtU5UUaLi1GeBUxcuq4/j4Xo811NiJmlb+QWKCw1+ukvo1Z\nRBD4Sl3mH27Zdx7RCoEIVhdkRfVcorUxC7iXeObGmnT8uUQ6m/TeP07lHyeeQLg98HriC38y0V90\nYVZUexKLLH64LvMvpXMmEsOK35HOLQYaILKiejNwf13mvxrI+cNlRIKDma1H/OVxlbufnMruQmkl\nEemHrKguJkZpTa3L/M9peZIHiVTZS4jWyO7E0N9diHWuPgp8oC7zL2dF9WJiza3ziLWvflCX+UGp\npXIn8Ghd5jun95pMBMFdgc/yTMf6Nq0jz1LgOp1odZ1LDAa4m1gSZWdi0uT2RKf+ma0z8luusQPR\nf0p6z2PrMr+9zXHPhr8tIDkihn2eQ0oZnQ3c0RUYkoW0RH1i3f2u8oMtzAZWu/uDxF8te5jZZDOb\nTPzDXzXQeonImHIWMflvv7R9APGF/XViEt1viMzEbsTkw48TLYj/TIHko+m8zxIB4i1ZUW1GPH72\nZcA3ut4oPU52L2I9rGOIPoaizZDkp4nlU0pixNcPieCwkvgeOzTV8Xjggayozu0+I50IZqt4Zr7K\nbVlRndT6bJHUWX8n8POsqDZei3+zETGYeQ47ESth7mpmv0w/exP/oG8ys6VExC/T8VcQIxaWER/Y\n4QDuvpL4YG9KP8elMhEZ/64mhry/O20fSvQf/Lwu88eJ+R+/AT5Ul/kFLWtlTSG+W94DfCt1xn8d\nWC9d493Ab4m/2v8mBYJ3E08SPIPoZ+khLeR4FDEH5Y3pnE8SrY5N6jLfgXhA1NnEBMOLsngGOVlR\nvZp4YNV/1WX+KeIhUmcTkxsXZEW1YVZUWxH9tM9L1zmrdVJiVlSvSx35o0bLZ4jIqMqK6mjiD8Q3\nEx3bH63L/OSW/dY975/SUfsTw9+37erQzorqGmJexRTg3LrMDx+B+h9GtICOqcv8+KyoriCG/WZd\n81nSF/9HiBFhNxABw4gW0T7A59L+04hWzTFp/+vqMr9hKOur5TNEZKw4j3ho0oXE2k/fbN3ZoUP4\n6HTs91pHOhGtgWnEMidnDUdl2+hatv0zWVF9gkhd/d0ij6kl8l9EQNueuN831GV+GzFb/bvEsvI/\nBY4l1u5dTxpPAAAFM0lEQVRaDpyTFdUGXdfJiupZaTjzsFNwEJFRVZf5cqJzdxKwsHX2dy/nLAVe\nS6SVWl0OPAQsqct8yVDXtUNdnOi8Xkakuh4FvtLh2G8TfR3b12V+Z8v5h6TztyVGaL2TGJn5j0Qr\ngtSv8d9EH8X0YbwlQGklEVkHZEWVEyMf59RlPqgBKWmhwT+2LgsyEtKEuh8Ts6zPGMD5k4EN6jJ/\nqKXsXKJv9wPEUNwpRCf8VwY6xHbEZ0iPNAUHkfElK6ot6zJfNtr1GIysqNbrtPjiAK83meigfyGx\nbMkBdZn/cjDXVHAQERkHsqLamViT7sS6zH8/2OspOIiISA8arSQiIgOm4CAiIj0oOIiISA8KDiIi\n0oOCg4iI9KDgICIiPSg4iIhIDwoOIiLSw1ieBPcocN8AT59CPES9SZp4z9DM+27iPUMz73sg9/wS\nd/+Hvg4as8FhMJo4u7qJ9wzNvO8m3jM0876H856VVhIRkR4UHEREpIemBoczR7sCo6CJ9wzNvO8m\n3jM0876H7Z4b2ecgIiK9a2rLQUREetGo4GBmc8zsLjNbZmbFaNdnuJjZdDO71sxuN7Nfm9kHU/km\nZrbIzJam35NHu65DzcwmmNnNZvb9tD3DzG5Mn/nFZrb+aNdxqJnZJDO71MzuNLM7zOy14/2zNrMP\np/+2bzOzi8xsw/H4WZvZOWb2iJnd1lLW9rO1cFq6/1vMbIfBvHdjgoOZTSAe+r0XsA1wkJltM7q1\nGjZPAR91922A2cAR6V4L4Gp3nwlcnbbHmw8Cd7RsnwSc4u5bAquAw0alVsPrVOAH7r418Ari/sft\nZ21mU4lnKs9y9+2ACcCBjM/P+jxgTreyTp/tXsDM9DMfWOvnWLdqTHAAdgSWufs97r4GWADMHeU6\nDQt3f9Ddl6TXvyO+LKYS93t+Oux8YL/RqeHwMLNpQA6clbYN2BW4NB0yHu/5+cDrgbMB3H2Nuz/B\nOP+sgYnAs81sIrAR8CDj8LN29+uAld2KO322c4ELPNwATDKzzQf63k0KDlOBB1q2l6eycc3MMmB7\n4EZgM3d/MO16CNhslKo1XL4EHAk8nbY3BZ5w96fS9nj8zGcAjwLnpnTaWWb2HMbxZ+3uK4AvAvcT\nQWE18AvG/2fdpdNnO6TfcU0KDo1jZhsD3wE+5O6/bd3nMUxt3AxVM7N9gEfc/RejXZcRNhHYATjD\n3bcH/kC3FNI4/KwnE38lzwBeBDyHnqmXRhjOz7ZJwWEFML1le1oqG5fMbD0iMHzL3S9LxQ93NTPT\n70dGq37DYCdgXzOriZThrkQuflJKPcD4/MyXA8vd/ca0fSkRLMbzZ707cK+7P+rufwEuIz7/8f5Z\nd+n02Q7pd1yTgsNNwMw0omF9ogNr4SjXaVikXPvZwB3ufnLLroXAvPR6HnD5SNdtuLj7Ue4+zd0z\n4rO9xt3fDlwLvDUdNq7uGcDdHwIeMLOtUtFuwO2M48+aSCfNNrON0n/rXfc8rj/rFp0+24XAwWnU\n0mxgdUv6aa01ahKcme1N5KUnAOe4+wmjXKVhYWY7Az8BbuWZ/PsniX6HS4AXEyva7u/u3Tu7xjwz\n2wX4mLvvY2ZbEC2JTYCbgXe4+5OjWb+hZmavJDrh1wfuAd5F/OE3bj9rM/sMcAAxMu9m4N1Efn1c\nfdZmdhGwC7H66sPAscD3aPPZpkB5OpFi+yPwLndfPOD3blJwEBGR/mlSWklERPpJwUFERHpQcBAR\nkR4UHEREpAcFBxER6UHBQUREelBwEBGRHhQcRESkh/8P4Oox4As2z8kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5749cd9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "loss_fn = entro.cross_entro_loss\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = batch_size, dim, 100, 6\n",
    "\n",
    "learning_rate = 1e-4\n",
    "losses = []\n",
    "batch_idx = 0\n",
    "\n",
    "x = Variable(torch.FloatTensor(train_vecs[batch_idx]).cuda(), requires_grad=True)\n",
    "\n",
    "yhot = np.zeros((y_batches[batch_idx].shape[0], D_out))\n",
    "yhot[[idx for idx in range(yhot.shape[0])], y_batches[batch_idx]] = 1\n",
    "y = Variable(torch.DoubleTensor(yhot), requires_grad=False)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = Variable(torch.zeros(1), requires_grad=True).double()\n",
    "    for i in range(len(train_vecs[batch_idx])):\n",
    "        yhat = y_pred[i] + y_pred[i].min().abs()\n",
    "        norm = torch.norm(yhat, 2)\n",
    "        yhat = yhat.div(norm)\n",
    "        yhat = yhat.div(yhat.sum())\n",
    "        yhat = yhat.double()\n",
    "\n",
    "        loss += loss_fn(yhat, y[i])\n",
    "\n",
    "    print ('epoch', str(epoch), 'loss', loss.data[0])\n",
    "    losses.append(loss.data[0])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
