{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe, CharNGram\n",
    "\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from minimax_entropy import MinimaxEntropyEstimator\n",
    "\n",
    "# https://github.com/pytorch/text\n",
    "# http://cogcomp.org/Data/QA/QC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entro = MinimaxEntropyEstimator('poly_coeff_entro.mat', gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=[GloVe(name='42B', dim=str(dim)), CharNGram()])\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, test_iter = datasets.TREC.iters(batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's start off padding the rest to be zero's to get to 300 dim\n",
    "def list_batches(t_iter, cap=100):\n",
    "    batches_text, batches_label = [], []\n",
    "    ct = 0\n",
    "    batch = next(iter(train_iter))\n",
    "    while batch and ct < 100:\n",
    "        batches_text.append(batch.text)\n",
    "        batches_label.append(batch.label)\n",
    "        batch = next(iter(train_iter))\n",
    "        ct += 1\n",
    "#         print (ct, batch.text.data.size())\n",
    "    return batches_text, batches_label\n",
    "\n",
    "train_text, train_label = list_batches(train_iter)\n",
    "test_text, test_label = list_batches(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = np.zeros((dim, 1))\n",
    "# x[:test_text[:,1].size()] = test_text[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "   36     4     4    14     4     4     4    48  1707     4     4     4     4\n",
      "   21    37    85   717     5    15    15  1737     5    11    10   563  8086\n",
      " 2909    20     6   197    19   159  4315   738     8     3     3     6   173\n",
      "  839  1262  1066   947     3   220    17   129  3345  1116  1118   195  6187\n",
      "  361  1229     5     3  9343  7385  6058  3158     6     6   209  8325     3\n",
      "    7   118     8  2096  9320   566    89   131    38  5143     6    15   143\n",
      " 2465    32  6820  3500    18  1152    16   648    37  5604  3843    50  8882\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "\n",
      "Columns 13 to 25 \n",
      "    4     4     4     9    14     9    25    14    36     9     4     4     4\n",
      "  108     5    20    22     5    23     5    11    21    23  7438     5    37\n",
      "   56  2885     3    30     3    54     3     3     3  1047   120     3    45\n",
      " 1651  6794  6637    55    19    21   297    28    61    15    22  1683     3\n",
      " 1258     8  4434  8610   390   242  2124  2456   572     7   496  1694   143\n",
      "   10  9041    96    24   794   240   872   581    92     8   106   135  6322\n",
      "  599     6    16  9407    18  7751  2111  1241   361  3521     7  1121   322\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "\n",
      "Columns 26 to 38 \n",
      "    9    14    52     4    14     4     9     4    14     4     9     4     4\n",
      "   23    10    40     5    11     5    23    20   125    37    23     5     5\n",
      "  263     3   459     3     3     3  1514  2152     3    45  1448  1645     3\n",
      "  765   109  7416   234  1995    19    20  2197  8645     3    15  1602   314\n",
      " 8097   137     7     6     6  2431    40    62    44    65     7    10  1408\n",
      "   15   589    86  1207  1149  4442  8108     7  1374  3435     8   547    16\n",
      "   50     7   188   241  1134    18    43   286     7   163   616    26  8037\n",
      "    2   290    47     2     2     2     2     2     2     2     2     2     2\n",
      "\n",
      "Columns 39 to 51 \n",
      "    9     4    36    14     4     4    25     4    25     4    14     9     4\n",
      "   23  1224    21     5     5     5    22     5     5    11    79    23    11\n",
      " 3090    46  4677     3     3   657    54   657   793   357  3922    54  2522\n",
      "   15   885   459   369    19  1798   211  1798    10    26    41    21   245\n",
      "   24    74    17   363  6824    10  6692    10  2927    16    40   242    10\n",
      "    8   485  2777     6    18   888     7   888   722   573  1081   240   130\n",
      " 2982  3079  3059   507  1340   131  5306   131   171   262  3449  1112    84\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "\n",
      "Columns 52 to 63 \n",
      "    4    31    25    36    36    14     4     4     4     9     4    25\n",
      "    5   781     5    21     5     5   480     5     5    11     5    22\n",
      "    3   686     3   520     3     3  6500  4435     3  1790     3     3\n",
      "   44     5   383   308  5967  1695   522    10  8387   802  1501  4043\n",
      "   10    89   514  2448  1656  1687   226  2161   212  3372     6  7875\n",
      "  143    41   307   130     7     7    10  8977     6    12   105   115\n",
      " 1468    38   171  3028  1727  8504  3576  1482   665  4515  3032  3271\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2\n",
      "[torch.cuda.LongTensor of size 8x64 (GPU 0)]\n",
      " Variable containing:\n",
      " 4\n",
      " 5\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 5\n",
      " 3\n",
      " 2\n",
      " 5\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 6\n",
      " 3\n",
      " 2\n",
      " 4\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 4\n",
      " 4\n",
      " 5\n",
      " 5\n",
      " 4\n",
      " 2\n",
      " 5\n",
      " 4\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 3\n",
      " 2\n",
      " 5\n",
      " 4\n",
      " 2\n",
      " 1\n",
      " 4\n",
      " 5\n",
      " 4\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 4\n",
      " 5\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 1\n",
      " 5\n",
      " 1\n",
      " 5\n",
      " 4\n",
      " 4\n",
      " 2\n",
      " 2\n",
      " 3\n",
      " 5\n",
      " 3\n",
      " 3\n",
      " 5\n",
      "[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def forward(self, inp, hidden, lengths):\n",
    "    embeddings = self.encoder(inp)\n",
    "    packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "    output, hidden = self.rnn(packed, hidden)\n",
    "    output, _ = pad_packed_sequence(output, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(len(test_label)):\n",
    "#     print (np.unique(test_label[i].data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = batch_size, dim, 100, 6\n",
    "\n",
    "print (test_text[0].size(), test_label[0].size())\n",
    "# # Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "# x = Variable(torch.randn(N, D_in))\n",
    "# y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# # Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# # is a Module which contains other Modules, and applies them in sequence to\n",
    "# # produce its output. Each Linear Module computes output from input using a\n",
    "# # linear function, and holds internal Variables for its weight and bias.\n",
    "# model = torch.nn.Sequential(\n",
    "#           torch.nn.Linear(D_in, H),\n",
    "#           torch.nn.ReLU(),\n",
    "#           torch.nn.Linear(H, D_out),\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout=0.):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(dropout)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "        self._dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self._dropout, training=self.training)\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 207.984918\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 199.785011\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 174.420898\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 134.108899\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 79.123192\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 61.239456\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 56.735480\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 73.298097\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 61.001399\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 68.880989\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 40.949460\n",
      "\n",
      "Test set: Average loss: -0.8354, Accuracy: 9068/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(dropout=dropout)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    target_onehot = Variable(torch.DoubleTensor(batch_size, 10)).cuda()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target_onehot.data.zero_()\n",
    "        for i in range(batch_size):\n",
    "            target_onehot[i, target.data[i]] = 1.\n",
    "        \n",
    "        output = model(data).double()\n",
    "        \n",
    "        loss = Variable(torch.zeros(1)).double().cuda()\n",
    "        for i in range(batch_size):\n",
    "            loss += entro.cross_entro_loss(output[i], target_onehot[i])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "        \n",
    "        if batch_idx == 100:\n",
    "            break\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
